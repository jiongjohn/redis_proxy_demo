package cache

import (
	"context"
	"encoding/json"
	"fmt"
	"os"
	"sync"
	"time"

	"github.com/segmentio/kafka-go"

	"redis-proxy-demo/config"
	"redis-proxy-demo/lib/logger"
)

// CacheEventType ç¼“å­˜äº‹ä»¶ç±»å‹
type CacheEventType string

const (
	CacheInvalidate CacheEventType = "invalidate" // ç¼“å­˜å¤±æ•ˆ
	CacheUpdate     CacheEventType = "update"     // ç¼“å­˜æ›´æ–°
	CacheClear      CacheEventType = "clear"      // æ¸…ç©ºç¼“å­˜
)

// CacheEvent ç¼“å­˜äº‹ä»¶æ¶ˆæ¯
type CacheEvent struct {
	Type      CacheEventType `json:"type"`      // äº‹ä»¶ç±»å‹
	Key       string         `json:"key"`       // Redisé”®
	Command   string         `json:"command"`   // è§¦å‘çš„Rediså‘½ä»¤
	Timestamp int64          `json:"timestamp"` // æ—¶é—´æˆ³
	Source    string         `json:"source"`    // äº‹ä»¶æºï¼ˆå®ä¾‹IDï¼‰
	Hostname  string         `json:"hostname"`  // æœºå™¨hostname
	Database  int            `json:"database"`  // redisæ•°æ®åº“
}

// CacheManager ç¼“å­˜ç®¡ç†å™¨æ¥å£
type CacheManager interface {
	InvalidateCache(key string, database int)
	Clear()
	ProcessRemoteUpdate(key string, database int)
}

// KafkaProducer Kafkaç”Ÿäº§è€…
type KafkaProducer struct {
	writer   *kafka.Writer
	hostname string
}

// NewKafkaProducer åˆ›å»ºKafkaç”Ÿäº§è€…
func NewKafkaProducer(cfg *config.Config) (*KafkaProducer, error) {
	if !cfg.Kafka.Enabled {
		return nil, fmt.Errorf("kafka is not enabled")
	}

	hostname, _ := os.Hostname()
	if hostname == "" {
		hostname = "unknown"
	}

	writer := &kafka.Writer{
		Addr:         kafka.TCP(cfg.Kafka.Brokers...),
		Topic:        cfg.Kafka.Topic,
		Balancer:     &kafka.LeastBytes{},
		RequiredAcks: kafka.RequireOne,
		Async:        false,
	}

	return &KafkaProducer{
		writer:   writer,
		hostname: hostname,
	}, nil
}

// PublishEvent å‘å¸ƒç¼“å­˜äº‹ä»¶
func (kp *KafkaProducer) PublishEvent(eventType CacheEventType, key string, database int, command, instanceID string) error {
	event := &CacheEvent{
		Type:      eventType,
		Key:       key,
		Command:   command,
		Timestamp: time.Now().UnixNano(),
		Source:    instanceID,
		Hostname:  kp.hostname,
		Database:  database,
	}

	// åºåˆ—åŒ–äº‹ä»¶
	eventData, err := json.Marshal(event)
	if err != nil {
		return fmt.Errorf("failed to marshal cache event: %w", err)
	}

	// å‘é€åˆ°Kafka
	message := kafka.Message{
		Key:   []byte(key),
		Value: eventData,
		Time:  time.Now(),
	}

	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	err = kp.writer.WriteMessages(ctx, message)
	if err != nil {
		return fmt.Errorf("failed to publish cache event to kafka: %w", err)
	}

	logger.Debugf("Published cache event: type=%s, key=%s, command=%s", eventType, key, command)
	return nil
}

// Close å…³é—­ç”Ÿäº§è€…
func (kp *KafkaProducer) Close() error {
	if kp.writer != nil {
		return kp.writer.Close()
	}
	return nil
}

// KafkaConsumer Kafkaæ¶ˆè´¹è€…
type KafkaConsumer struct {
	reader       *kafka.Reader
	cacheManager CacheManager
	hostname     string
	instanceID   string
	stopChan     chan struct{}
	wg           sync.WaitGroup
	running      bool
	mu           sync.RWMutex
}

// NewKafkaConsumer åˆ›å»ºKafkaæ¶ˆè´¹è€…
func NewKafkaConsumer(cfg *config.Config, cacheManager CacheManager, instanceID string) (*KafkaConsumer, error) {
	if !cfg.Kafka.Enabled {
		return nil, fmt.Errorf("kafka is not enabled")
	}

	hostname, _ := os.Hostname()
	if hostname == "" {
		hostname = "unknown"
	}

	// ç”Ÿæˆå¸¦hostnameçš„æ¶ˆè´¹è€…ç»„ID
	consumerGroupID := fmt.Sprintf("%s-%s", cfg.Kafka.GroupID, hostname)

	reader := kafka.NewReader(kafka.ReaderConfig{
		Brokers:        cfg.Kafka.Brokers,
		Topic:          cfg.Kafka.Topic,
		GroupID:        consumerGroupID,
		StartOffset:    kafka.LastOffset,
		CommitInterval: time.Second,
		MaxBytes:       10e6,
	})

	return &KafkaConsumer{
		reader:       reader,
		cacheManager: cacheManager,
		hostname:     hostname,
		instanceID:   instanceID,
		stopChan:     make(chan struct{}),
	}, nil
}

// Start å¯åŠ¨æ¶ˆè´¹è€…
func (kc *KafkaConsumer) Start() error {
	kc.mu.Lock()
	defer kc.mu.Unlock()

	if kc.running {
		return fmt.Errorf("kafka consumer is already running")
	}

	kc.running = true
	kc.wg.Add(1)
	go kc.consumeMessages()

	logger.Infof("Kafka consumer started for instance: %s", kc.instanceID)
	return nil
}

// Stop åœæ­¢æ¶ˆè´¹è€…
func (kc *KafkaConsumer) Stop() error {
	kc.mu.Lock()
	defer kc.mu.Unlock()

	if !kc.running {
		return nil
	}

	kc.running = false
	close(kc.stopChan)
	kc.wg.Wait()

	if err := kc.reader.Close(); err != nil {
		logger.Errorf("Failed to close Kafka reader: %v", err)
		return err
	}

	logger.Info("Kafka consumer stopped")
	return nil
}

// consumeMessages æ¶ˆè´¹Kafkaæ¶ˆæ¯
func (kc *KafkaConsumer) consumeMessages() {
	defer kc.wg.Done()

	logger.Infof("Started consuming cache events from Kafka topic: %s", kc.reader.Config().Topic)

	for {
		select {
		case <-kc.stopChan:
			logger.Info("Stopping Kafka message consumption")
			return
		default:
			ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
			message, err := kc.reader.ReadMessage(ctx)
			cancel()

			if err != nil {
				if err == context.DeadlineExceeded || err == context.Canceled {
					continue
				}
				logger.Errorf("Failed to read message from Kafka: %v", err)
				continue
			}

			// å¤„ç†æ¶ˆæ¯
			if err := kc.processMessage(&message); err != nil {
				logger.Errorf("Failed to process cache event: %v", err)
			}
		}
	}
}

// processMessage å¤„ç†Kafkaæ¶ˆæ¯
func (kc *KafkaConsumer) processMessage(message *kafka.Message) error {
	// ååºåˆ—åŒ–äº‹ä»¶
	var event CacheEvent
	if err := json.Unmarshal(message.Value, &event); err != nil {
		return fmt.Errorf("failed to unmarshal cache event: %w", err)
	}

	// å¦‚æœæ˜¯æœ¬æœºå‘é€çš„æ¶ˆæ¯ï¼Œè·³è¿‡å¤„ç†
	if event.Hostname == kc.hostname {
		logger.Debugf("Skipping event from same hostname: key=%s, hostname=%s", event.Key, event.Hostname)
		return nil
	}

	logger.Debugf("Processing cache event: type=%s, key=%s, command=%s, source=%s",
		event.Type, event.Key, event.Command, event.Source)

	// å¤„ç†äº‹ä»¶
	switch event.Type {
	case CacheInvalidate:
		logger.Debugf("Invalidating cache for key: %s", event.Key)
		kc.cacheManager.InvalidateCache(event.Key, event.Database)

	case CacheUpdate:
		// å¯¹äºæ›´æ–°äº‹ä»¶ï¼Œç›´æ¥ä½¿ç”¨æ¶ˆæ¯ä¸­çš„valueæ›´æ–°æœ¬åœ°ç¼“å­˜
		logger.Debugf("Processing remote cache update for key: %s ", event.Key)
		kc.cacheManager.ProcessRemoteUpdate(event.Key, event.Database)

	case CacheClear:
		logger.Debug("Clearing all cache")
		kc.cacheManager.Clear()

	default:
		return fmt.Errorf("unknown cache event type: %s", event.Type)
	}

	return nil
}

// IsRunning æ£€æŸ¥æ¶ˆè´¹è€…æ˜¯å¦è¿è¡Œä¸­
func (kc *KafkaConsumer) IsRunning() bool {
	kc.mu.RLock()
	defer kc.mu.RUnlock()
	return kc.running
}

// GetStats è·å–ç»Ÿè®¡ä¿¡æ¯
func (kc *KafkaConsumer) GetStats() map[string]interface{} {
	kc.mu.RLock()
	defer kc.mu.RUnlock()

	consumerGroupID := fmt.Sprintf("%s-%s", kc.reader.Config().GroupID, kc.hostname)

	return map[string]interface{}{
		"running":           kc.running,
		"instance_id":       kc.instanceID,
		"hostname":          kc.hostname,
		"topic":             kc.reader.Config().Topic,
		"consumer_group_id": consumerGroupID,
		"brokers":           kc.reader.Config().Brokers,
	}
}

// CacheUpdateRequest ç¼“å­˜æ›´æ–°è¯·æ±‚
type CacheUpdateRequest struct {
	Operation string
	Key       string // Redisé”®
	Command   string // è§¦å‘çš„Rediså‘½ä»¤
	Database  int
}

// ConsistentCache ä¸€è‡´æ€§ç¼“å­˜
type ConsistentCache struct {
	cache      *SmartCache
	producer   *KafkaProducer
	consumer   *KafkaConsumer
	instanceID string
	enabled    bool
	mu         sync.RWMutex

	// å¼‚æ­¥æ›´æ–°ç›¸å…³
	updateChan  chan CacheUpdateRequest
	workerCount int
	ctx         context.Context
	cancel      context.CancelFunc
	wg          sync.WaitGroup
}

// NewConsistentCache åˆ›å»ºä¸€è‡´æ€§ç¼“å­˜
func NewConsistentCache(cfg *config.Config, instanceID string) (*ConsistentCache, error) {
	// åˆ›å»ºæ™ºèƒ½ç¼“å­˜
	cache, err := NewSmartCache(cfg)
	if err != nil {
		return nil, fmt.Errorf("failed to create smart cache: %w", err)
	}

	// åˆ›å»ºcontextç”¨äºä¼˜é›…å…³é—­
	ctx, cancel := context.WithCancel(context.Background())

	cc := &ConsistentCache{
		cache:       cache,
		instanceID:  instanceID,
		enabled:     cfg.Kafka.Enabled,
		updateChan:  make(chan CacheUpdateRequest, 1000), // ç¼“å†²1000ä¸ªæ›´æ–°è¯·æ±‚
		workerCount: 3,                                   // å¯åŠ¨3ä¸ªworker goroutine
		ctx:         ctx,
		cancel:      cancel,
	}

	// å¦‚æœKafkaå¯ç”¨ï¼Œåˆ›å»ºç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…
	if cfg.Kafka.Enabled {
		producer, err := NewKafkaProducer(cfg)
		if err != nil {
			logger.Errorf("Failed to create Kafka producer: %v", err)
			// ä¸è¿”å›é”™è¯¯ï¼Œç»§ç»­ä½¿ç”¨æœ¬åœ°ç¼“å­˜
		} else {
			cc.producer = producer
		}

		consumer, err := NewKafkaConsumer(cfg, cache, instanceID)
		if err != nil {
			logger.Errorf("Failed to create Kafka consumer: %v", err)
			// ä¸è¿”å›é”™è¯¯ï¼Œç»§ç»­ä½¿ç”¨æœ¬åœ°ç¼“å­˜
		} else {
			cc.consumer = consumer
			// å¯åŠ¨æ¶ˆè´¹è€…
			if err := cc.consumer.Start(); err != nil {
				logger.Errorf("Failed to start Kafka consumer: %v", err)
			}
		}
	}

	// å¯åŠ¨å¼‚æ­¥ç¼“å­˜æ›´æ–°worker goroutines
	cc.startAsyncWorkers()

	logger.Infof("âœ… å¼‚æ­¥ç¼“å­˜æ›´æ–°å·²å¯ç”¨ (Workers: %d, Buffer: %d)", cc.workerCount, cap(cc.updateChan))

	return cc, nil
}

// startAsyncWorkers å¯åŠ¨å¼‚æ­¥ç¼“å­˜æ›´æ–°worker goroutines
func (cc *ConsistentCache) startAsyncWorkers() {
	for i := 0; i < cc.workerCount; i++ {
		cc.wg.Add(1)
		go cc.asyncCacheWorker(i)
	}
	logger.Debugf("Started %d async cache workers", cc.workerCount)
}

// asyncCacheWorker å¼‚æ­¥ç¼“å­˜æ›´æ–°worker
func (cc *ConsistentCache) asyncCacheWorker(workerID int) {
	defer cc.wg.Done()

	logger.Debugf("Async cache worker %d started", workerID)

	for {
		select {
		case <-cc.ctx.Done():
			logger.Debugf("Async cache worker %d shutting down", workerID)
			return

		case req := <-cc.updateChan:
			cc.processAsyncCacheUpdate(workerID, req)
		}
	}
}

// processAsyncCacheUpdate å¤„ç†å¼‚æ­¥ç¼“å­˜æ›´æ–°
func (cc *ConsistentCache) processAsyncCacheUpdate(workerID int, req CacheUpdateRequest) {
	start := time.Now()

	switch req.Operation {
	case "SET":
		// æ›´æ–°æœ¬åœ°ç¼“å­˜
		cc.cache.ProcessSET(req.Key, req.Database)

		// å‘å¸ƒKafkaäº‹ä»¶
		if cc.enabled && cc.producer != nil {
			if err := cc.producer.PublishEvent(CacheUpdate, req.Key, req.Database, req.Command, cc.instanceID); err != nil {
				logger.Errorf("Worker %d failed to publish cache update event: %v", workerID, err)
			}
		}

	case "INVALIDATE":
		// æœ¬åœ°ç¼“å­˜å¤±æ•ˆ
		cc.cache.InvalidateCache(req.Key, req.Database)

		// å¹¿æ’­å¤±æ•ˆäº‹ä»¶
		if cc.enabled && cc.producer != nil {
			if err := cc.producer.PublishEvent(CacheInvalidate, req.Key, req.Database, "INVALIDATE", cc.instanceID); err != nil {
				logger.Errorf("Worker %d failed to publish cache invalidate event: %v", workerID, err)
			}
		}

	case "CLEAR":
		// æœ¬åœ°ç¼“å­˜æ¸…ç©º
		cc.cache.Clear()

		// å¹¿æ’­æ¸…ç©ºäº‹ä»¶
		if cc.enabled && cc.producer != nil {
			if err := cc.producer.PublishEvent(CacheClear, "", req.Database, "FLUSHALL", cc.instanceID); err != nil {
				logger.Errorf("Worker %d failed to publish cache clear event: %v", workerID, err)
			}
		}

	default:
		logger.Errorf("Worker %d received unknown cache operation: %s", workerID, req.Operation)
		return
	}

	duration := time.Since(start)
	logger.Debugf("Worker %d processed %s operation for key '%s' in %v",
		workerID, req.Operation, req.Key, duration)
}

// ProcessGET å¤„ç†GETå‘½ä»¤
func (cc *ConsistentCache) ProcessGET(key string, database int) (interface{}, bool) {
	return cc.cache.ProcessGET(key, database)
}

// ProcessSET å¤„ç†SETå‘½ä»¤å¹¶å¼‚æ­¥å¹¿æ’­
func (cc *ConsistentCache) ProcessSET(key string, command string, database int) {
	// æ£€æŸ¥keyæ˜¯å¦åº”è¯¥è¢«ç¼“å­˜
	if !cc.cache.ShouldCacheKey(key) {
		logger.Debugf("Key %s matches no-cache prefix, skipping SET processing", key)
		return
	}

	operation := "SET"
	// å¼‚æ­¥æäº¤ç¼“å­˜æ›´æ–°è¯·æ±‚
	select {
	case cc.updateChan <- CacheUpdateRequest{
		Operation: operation,
		Key:       key,
		Command:   command,
		Database:  database,
	}:
		// æˆåŠŸæäº¤åˆ°å¼‚æ­¥é˜Ÿåˆ—
		logger.Debugf("Async SET request queued for key: %s", key)
	default:
		// é˜Ÿåˆ—æ»¡äº†ï¼ŒåŒæ­¥å¤„ç†ä»¥é¿å…ä¸¢å¤±
		logger.Warn(fmt.Sprintf("Async queue full, processing SET synchronously for key: %s", key))
		cc.cache.ProcessSET(key, database)
		if cc.enabled && cc.producer != nil {
			if err := cc.producer.PublishEvent(CacheUpdate, key, database, "SET", cc.instanceID); err != nil {
				logger.Errorf("Failed to publish cache update event: %v", err)
			}
		}
	}
}

// InvalidateCache ä½¿ç¼“å­˜å¤±æ•ˆå¹¶å¼‚æ­¥å¹¿æ’­
func (cc *ConsistentCache) InvalidateCache(key string, database int) {
	// æ£€æŸ¥keyæ˜¯å¦åº”è¯¥è¢«ç¼“å­˜
	if !cc.cache.ShouldCacheKey(key) {
		logger.Debugf("Key %s matches no-cache prefix, skipping INVALIDATE processing", key)
		return
	}

	// å¼‚æ­¥æäº¤ç¼“å­˜å¤±æ•ˆè¯·æ±‚
	select {
	case cc.updateChan <- CacheUpdateRequest{
		Key:       key,
		Command:   "DEL",
		Operation: "INVALIDATE",
		Database:  database,
	}:
		// æˆåŠŸæäº¤åˆ°å¼‚æ­¥é˜Ÿåˆ—
		logger.Debugf("Async INVALIDATE request queued for key: %s database:%d", key, database)
	default:
		// é˜Ÿåˆ—æ»¡äº†ï¼ŒåŒæ­¥å¤„ç†ä»¥é¿å…ä¸¢å¤±
		logger.Warn(fmt.Sprintf("Async queue full, processing INVALIDATE synchronously for key: %s", key))
		cc.cache.InvalidateCache(key, database)
		if cc.enabled && cc.producer != nil {
			if err := cc.producer.PublishEvent(CacheInvalidate, key, database, "INVALIDATE", cc.instanceID); err != nil {
				logger.Errorf("Failed to publish cache invalidate event: %v", err)
			}
		}
	}
}

// Clear æ¸…ç©ºç¼“å­˜å¹¶å¼‚æ­¥å¹¿æ’­
func (cc *ConsistentCache) Clear() {
	// å¼‚æ­¥æäº¤ç¼“å­˜æ¸…ç©ºè¯·æ±‚
	select {
	case cc.updateChan <- CacheUpdateRequest{
		Command:   "FLUSHALL",
		Operation: "CLEAR",
	}:
		// æˆåŠŸæäº¤åˆ°å¼‚æ­¥é˜Ÿåˆ—
		logger.Debugf("Async CLEAR request queued")
	default:
		// é˜Ÿåˆ—æ»¡äº†ï¼ŒåŒæ­¥å¤„ç†ä»¥é¿å…ä¸¢å¤±
		logger.Warn("Async queue full, processing CLEAR synchronously")
		cc.cache.Clear()
		if cc.enabled && cc.producer != nil {
			if err := cc.producer.PublishEvent(CacheClear, "", 0, "FLUSHALL", cc.instanceID); err != nil {
				logger.Errorf("Failed to publish cache clear event: %v", err)
			}
		}
	}
}

// GetStats è·å–ç»Ÿè®¡ä¿¡æ¯
func (cc *ConsistentCache) GetStats() map[string]interface{} {
	cc.mu.RLock()
	defer cc.mu.RUnlock()

	stats := cc.cache.GetStats()
	stats["instance_id"] = cc.instanceID
	stats["kafka_enabled"] = cc.enabled

	if cc.consumer != nil {
		consumerStats := cc.consumer.GetStats()
		for k, v := range consumerStats {
			stats["kafka_"+k] = v
		}
	}

	return stats
}

// Close å…³é—­ä¸€è‡´æ€§ç¼“å­˜
func (cc *ConsistentCache) Close() error {
	cc.mu.Lock()
	defer cc.mu.Unlock()

	var errs []error

	// 1. åœæ­¢æ¥æ”¶æ–°çš„å¼‚æ­¥è¯·æ±‚
	logger.Info("ğŸ”„ åœæ­¢å¼‚æ­¥ç¼“å­˜æ›´æ–°...")
	if cc.cancel != nil {
		cc.cancel()
	}

	// 2. ç­‰å¾…æ‰€æœ‰workerå®Œæˆå½“å‰å·¥ä½œ
	logger.Info("â³ ç­‰å¾…å¼‚æ­¥workerå®Œæˆ...")
	done := make(chan struct{})
	go func() {
		cc.wg.Wait()
		close(done)
	}()

	// ç­‰å¾…æœ€å¤š5ç§’è®©workerå®Œæˆ
	select {
	case <-done:
		logger.Info("âœ… æ‰€æœ‰å¼‚æ­¥workerå·²å®Œæˆ")
	case <-time.After(5 * time.Second):
		logger.Warn("âš ï¸ å¼‚æ­¥workerè¶…æ—¶ï¼Œå¼ºåˆ¶å…³é—­")
	}

	// 3. å¤„ç†å‰©ä½™çš„é˜Ÿåˆ—ä¸­çš„è¯·æ±‚
	close(cc.updateChan)
	remainingCount := 0
	for req := range cc.updateChan {
		cc.processAsyncCacheUpdate(-1, req) // ä½¿ç”¨-1è¡¨ç¤ºå…³é—­æ—¶çš„å¤„ç†
		remainingCount++
	}
	if remainingCount > 0 {
		logger.Infof("ğŸ§¹ å¤„ç†äº† %d ä¸ªå‰©ä½™çš„å¼‚æ­¥è¯·æ±‚", remainingCount)
	}

	// 4. åœæ­¢Kafkaæ¶ˆè´¹è€…
	if cc.consumer != nil {
		if err := cc.consumer.Stop(); err != nil {
			errs = append(errs, fmt.Errorf("failed to stop consumer: %w", err))
		}
	}

	// 5. å…³é—­Kafkaç”Ÿäº§è€…
	if cc.producer != nil {
		if err := cc.producer.Close(); err != nil {
			errs = append(errs, fmt.Errorf("failed to close producer: %w", err))
		}
	}

	// 6. å…³é—­æœ¬åœ°ç¼“å­˜
	if cc.cache != nil {
		if err := cc.cache.Close(); err != nil {
			errs = append(errs, fmt.Errorf("failed to close cache: %w", err))
		}
	}

	if len(errs) > 0 {
		return fmt.Errorf("multiple errors during close: %v", errs)
	}

	logger.Info("âœ… å¼‚æ­¥ç¼“å­˜ç³»ç»Ÿå·²å®Œå…¨å…³é—­")
	return nil
}
